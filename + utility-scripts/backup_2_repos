#!/usr/bin/env zsh

# CONFIG
backup_location="$DATA_DIR/Backups/My Repos"
max_number_of_bkps=4

# ignores perma-repos since they are already locally backed up
ignore_repos=$(cut -d, -f2 "$HOME/.config/perma-repos.csv" |
	cut -c2- |
	xargs basename |
	tr "\n" "|" |
	sed -e "s/.$//") # construct regex pattern for `grep`

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# GET ALL REPOS
# only non-forked, non-archived repos
repos=$(gh repo list --limit=200 --no-archived --source --json="nameWithOwner" --jq=".[].nameWithOwner" |
	grep --extended-regexp --invert-match "$ignore_repos")
repos_count=$(echo "$repos" | wc -l | tr -d " ")
if [[ repos_count -ge 100 ]]; then
	print "\e[1;33mMore than 100 repos. Unclear whether GitHub API allows for so many listings, so need to manually check that all repos are included.\e[0m"
fi

# DOWNLOAD REPOS
mkdir -p "$backup_location/temp"
cd "$backup_location/temp" || return 1
i=0
echo "$repos" | while read -r repo; do
	((i++))
	print "\e[1;34m$repo ($i/$repos_count)\e[0m"
	git clone --no-progress "git@github.com:$repo.git"
	echo
done

# ARCHIVE THEM
isodate=$(date +%Y-%m-%d)
archive_name="${repos_count} Repos â€“ ${isodate}.zip"
zip --recurse-paths --symlinks --quiet "../$archive_name" . || return 1

# RESTRICT NUMBER OF BACKUPS
cd "$backup_location" || return 1
# shellcheck disable=2012
ls -t | tail -n +$((max_number_of_bkps + 1)) | tr '\n' '\0' | xargs -0 rm
rm -rf "$backup_location/temp"

# NOTIFY
print "\e[1;32mArchived $repos_count repos.\e[0m"
"$ZDOTDIR/notificator" --title "ğŸŒRepo Backup" \
	--message "$repos_count repos archived." --sound "Blow"
